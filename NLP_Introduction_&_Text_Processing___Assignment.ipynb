{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Computational Linguistics and how does it relate to NLP?\n",
        "\n",
        "Computational Linguistics (CL) is an interdisciplinary field that combines computer science, linguistics, and artificial intelligence to study and model human language using computational methods. Its primary goal is to enable computers to understand, interpret, and generate human language in a meaningful and contextually appropriate way.\n",
        "\n",
        "It involves creating algorithms and models that can process linguistic data—such as grammar, syntax, semantics, and pragmatics—to analyze or produce natural language. Researchers in computational linguistics build formal representations of linguistic phenomena and test them using computational systems.\n",
        "\n",
        "Relation to Natural Language Processing (NLP):\n",
        "Computational linguistics forms the theoretical and scientific foundation of NLP. While CL focuses on understanding how language works and how it can be modeled computationally, NLP applies these theories to develop practical applications, such as machine translation, speech recognition, chatbots, and sentiment analysis.\n",
        "\n",
        "Computational Linguistics = Science (the study and modeling of language computationally)\n",
        "\n",
        "NLP = Engineering (the implementation and application of those models in real-world systems)\n",
        "\n",
        "Thus, NLP is a direct outcome of advances in computational linguistics, turning linguistic theory into intelligent, language-capable technologies."
      ],
      "metadata": {
        "id": "D3M4GQ1--7W7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Briefly describe the historical evolution of Natural Language Processing.\n",
        "\n",
        "The evolution of Natural Language Processing (NLP) reflects the progress of artificial intelligence in understanding human language. In the 1950s, pioneers like Alan Turing laid its foundation by proposing the Turing Test to evaluate machine intelligence. Early programs such as ELIZA (1966) simulated simple conversations through pattern matching but lacked real understanding. During the 1960s–1970s, researchers developed rule-based systems relying on manually crafted grammars and linguistic rules. Though effective for limited domains, they failed to handle ambiguity and real-world complexity.\n",
        "\n",
        "In the 1980s, the field shifted towards statistical and probabilistic models, fueled by increased computational power and linguistic data. Methods like Hidden Markov Models (HMMs) and n-gram models enhanced applications in speech and text processing. The 1990s–2000s saw the rise of machine learning with algorithms such as Support Vector Machines (SVMs) and Conditional Random Fields (CRFs) trained on large annotated corpora, improving accuracy in parsing and tagging tasks.\n",
        "\n",
        "The 2010s brought the deep learning revolution, introducing neural networks, RNNs, and Transformers like BERT and GPT, capable of capturing contextual and semantic nuances. Today, NLP powers advanced applications such as chatbots, translation, and sentiment analysis, bridging human communication with artificial intelligence."
      ],
      "metadata": {
        "id": "r3CJin-a_GrP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. List and explain three major use cases of NLP in today’s tech industry.\n",
        "\n",
        "Here are three major use cases of Natural Language Processing (NLP) in today’s tech industry:\n",
        "\n",
        "Chatbots and Virtual Assistants:\n",
        "NLP enables systems like ChatGPT, Siri, Alexa, and Google Assistant to understand and respond to human speech naturally. These applications use speech recognition, intent detection, and dialogue management to simulate human-like conversations. Businesses deploy chatbots for customer service, automating queries and improving user experience while reducing operational costs.\n",
        "\n",
        "Sentiment Analysis:\n",
        "Companies use NLP to analyze opinions and emotions expressed in social media posts, reviews, and surveys. Sentiment analysis algorithms classify text as positive, negative, or neutral, helping brands assess public perception, monitor reputation, and make data-driven marketing decisions. For instance, analyzing tweets about a new product can reveal customer satisfaction trends.\n",
        "\n",
        "Machine Translation:\n",
        "Tools like Google Translate and DeepL rely on NLP and deep learning to translate text between languages accurately. Modern translation systems use transformer architectures to preserve meaning, tone, and context across languages, facilitating global communication and accessibility.\n",
        "\n",
        "Overall, NLP drives automation, personalization, and cross-language understanding across industries, making technology more intuitive and human-centered."
      ],
      "metadata": {
        "id": "8D--xAht_SwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is text normalization and why is it essential in text processing tasks?\n",
        "\n",
        "Text normalization is the process of converting raw text into a standardized, consistent, and machine-readable format before analysis or modeling. It ensures that variations in text—caused by differences in case, spelling, punctuation, or formatting—do not distort the meaning or bias computational models.\n",
        "\n",
        "Normalization typically involves several steps, such as:\n",
        "\n",
        "Lowercasing: Converting all letters to lowercase (e.g., “Apple” → “apple”).\n",
        "\n",
        "Removing punctuation and special characters: Cleaning unnecessary symbols that don’t affect meaning.\n",
        "\n",
        "Expanding contractions: Turning “don’t” into “do not.”\n",
        "\n",
        "Lemmatization or stemming: Reducing words to their root or base form (e.g., “running” → “run”).\n",
        "\n",
        "Removing extra spaces or stopwords: Simplifying text for efficient processing.\n",
        "\n",
        "It is essential in text processing tasks because language data is inherently noisy and inconsistent. Without normalization, algorithms may treat semantically identical words (like “USA” and “U.S.A.”) as different tokens, reducing model accuracy. Proper normalization improves data uniformity, enhances feature extraction, and ensures that downstream NLP tasks—such as sentiment analysis, translation, or information retrieval—perform more accurately and efficiently. In short, it is the foundation of reliable and meaningful text analytics."
      ],
      "metadata": {
        "id": "IDi94_kA_bjO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare and contrast stemming and lemmatization with suitable\n",
        "examples.\n",
        "\n",
        "Stemming and lemmatization are both text normalization techniques used in Natural Language Processing (NLP) to reduce words to their base or root forms, but they differ in approach and accuracy.\n",
        "\n",
        "\n",
        "a. Stemming:\n",
        "\n",
        "\n",
        "Definition: A rule-based process that removes prefixes or suffixes from words to obtain their root form, often without considering grammatical correctness.\n",
        "\n",
        "\n",
        "Approach: Uses simple heuristics (like chopping off “-ing,” “-ed,” or “-s”) without understanding context or part of speech.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "“Running” → “run”\n",
        "\n",
        "\n",
        "“Studies” → “studi”\n",
        "\n",
        "\n",
        "“Better” → “bett”\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Advantages: Fast and computationally inexpensive.\n",
        "\n",
        "\n",
        "Disadvantages: Can produce non-words or incorrect stems due to its crude method.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "b. Lemmatization:\n",
        "\n",
        "\n",
        "Definition: A more sophisticated process that reduces words to their lemma (dictionary form) using vocabulary and morphological analysis.\n",
        "\n",
        "\n",
        "Approach: Considers the part of speech and context of the word.\n",
        "\n",
        "\n",
        "Example:\n",
        "\n",
        "\n",
        "“Running” → “run”\n",
        "\n",
        "\n",
        "“Studies” → “study”\n",
        "\n",
        "\n",
        "“Better” → “good”\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Advantages: Produces valid dictionary words and contextually accurate results.\n",
        "\n",
        "\n",
        "Disadvantages: Slower and requires linguistic resources like WordNet.\n",
        "\n",
        "\n",
        "Stemming is faster but less accurate, while lemmatization is linguistically informed and yields cleaner, more meaningful results—crucial for high-quality NLP applications."
      ],
      "metadata": {
        "id": "07nkoJnl_ko2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program that uses regular expressions (regex) to extract all\n",
        "email addresses from the following block of text:\n",
        "\n",
        "“Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”\n"
      ],
      "metadata": {
        "id": "lI3rsvUS_7tj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "y7B_Ar99-wwl"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Input text\n",
        "text = \"\"\"\n",
        "Hello team, please contact us at support@xyz.com for technical issues, or reach out to our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us.\n",
        "For partnership inquiries, email partners@xyz.biz.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "82Me-Vb4AJ0M"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regular expression pattern for emails\n",
        "pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Extract all email addresses\n",
        "emails = re.findall(pattern, text)"
      ],
      "metadata": {
        "id": "p3TirWm3ARat"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracted Email Addresses:\")\n",
        "for email in emails:\n",
        "    print(email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLeiyNmVAWBh",
        "outputId": "2779ef72-c71b-4091-ea23-04d78136d602"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Email Addresses:\n",
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Given the sample paragraph below, perform string tokenization and\n",
        "frequency distribution using Python and NLTK:\n",
        "\n",
        "“Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots,\n",
        "sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.”"
      ],
      "metadata": {
        "id": "3Z1mfrXRX3rr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist"
      ],
      "metadata": {
        "id": "WNZ7o0gpAa4R"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Input paragraph\n",
        "text = \"\"\"\n",
        "Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand,\n",
        "interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP\n",
        "in modern solutions is becoming increasingly critical.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "aF0DN9YiYFZx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57512010",
        "outputId": "3fc2b698-2cf1-40d5-cf6e-4f8e1e69d04c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "# Download required NLTK resources\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Frequency distribution\n",
        "freq_dist = FreqDist(tokens)"
      ],
      "metadata": {
        "id": "rarj7FR-YWwD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nFrequency Distribution (Top 10 Words):\")\n",
        "for word, freq in freq_dist.most_common(10):\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ4OnpMBYk8Z",
        "outputId": "44a31c45-3095-48d4-80d1-cc0a299f5442"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution (Top 10 Words):\n",
            ",: 7\n",
            ".: 4\n",
            "NLP: 3\n",
            "and: 3\n",
            "is: 2\n",
            "of: 2\n",
            "Natural: 1\n",
            "Language: 1\n",
            "Processing: 1\n",
            "(: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.  Create a custom annotator using spaCy or NLTK that identifies and labels\n",
        "proper nouns in a given text."
      ],
      "metadata": {
        "id": "hn_vIvjaY5MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "updn4jAiY29R"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Input text\n",
        "text = \"\"\"\n",
        "OpenAI developed ChatGPT, and Google created Bard.\n",
        "Elon Musk founded SpaceX and Tesla.\n",
        "Microsoft is headquartered in Redmond, Washington.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Q4FOTw6aZF0F"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process the text\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "psHG0nEXZJ5a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Proper Nouns Identified:\")\n",
        "for token in doc:\n",
        "    if token.pos_ == \"PROPN\":   # Check if token is a proper noun\n",
        "        print(f\"{token.text}  →  {token.pos_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGIdIatBZNjp",
        "outputId": "7620debc-aff2-47a6-8462-9f3a2bb60cef"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Nouns Identified:\n",
            "OpenAI  →  PROPN\n",
            "Google  →  PROPN\n",
            "Bard  →  PROPN\n",
            "Elon  →  PROPN\n",
            "Musk  →  PROPN\n",
            "SpaceX  →  PROPN\n",
            "Tesla  →  PROPN\n",
            "Microsoft  →  PROPN\n",
            "Redmond  →  PROPN\n",
            "Washington  →  PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Using Genism, demonstrate how to train a simple Word2Vec model on the\n",
        "following dataset consisting of example sentences:\n",
        "\n",
        "dataset = [\n",
        " \"Natural language processing enables computers to understand human language\",\n",
        " \"Word embeddings are a type of word representation that allows words with similar\n",
        "meaning to have similar representation\",\n",
        " \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        " \"Text preprocessing is a critical step before training word embeddings\",\n",
        " \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using Gensim.\n"
      ],
      "metadata": {
        "id": "AXk-5MFuZUpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnd2y0_JZqPj",
        "outputId": "ab734eb7-27cc-4697-b920-7596f17af779"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess"
      ],
      "metadata": {
        "id": "_6rxZzCIZP95"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample dataset\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]"
      ],
      "metadata": {
        "id": "s1VpAupbZihy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization and preprocessing\n",
        "processed_data = [simple_preprocess(sentence) for sentence in dataset]"
      ],
      "metadata": {
        "id": "vQIzMQGxZx5P"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train Word2Vec model\n",
        "model = Word2Vec(sentences=processed_data, vector_size=100, window=5, min_count=1, sg=1, epochs=100)"
      ],
      "metadata": {
        "id": "bPoo9sgbZ1qQ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore vocabulary\n",
        "print(\"Vocabulary:\", list(model.wv.key_to_index.keys()))\n",
        "print(\"\\nMost similar to 'word':\", model.wv.most_similar('word'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCaI5YECZ38G",
        "outputId": "b51fa37b-0ef3-499a-dabd-addc35040986"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['word', 'text', 'is', 'similar', 'representation', 'embeddings', 'to', 'language', 'modeling', 'for', 'raw', 'clean', 'help', 'normalization', 'and', 'tokenization', 'training', 'before', 'step', 'critical', 'preprocessing', 'applications', 'nlp', 'many', 'in', 'used', 'technique', 'embedding', 'popular', 'vec', 'have', 'meaning', 'with', 'words', 'allows', 'that', 'of', 'type', 'are', 'human', 'understand', 'computers', 'enables', 'processing', 'natural']\n",
            "\n",
            "Most similar to 'word': [('allows', 0.594346284866333), ('to', 0.5336982011795044), ('have', 0.5336185097694397), ('with', 0.5330169200897217), ('type', 0.4748493432998657), ('popular', 0.47382819652557373), ('representation', 0.4706820845603943), ('that', 0.4632415175437927), ('words', 0.44776225090026855), ('language', 0.4428447186946869)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you are a data scientist at a fintech startup. You’ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process, and extract useful insights using NLP techniques from thousands of customer reviews.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "As a data scientist analyzing customer feedback for a fintech startup, the goal is to transform unstructured text (reviews) into actionable insights. Here’s a structured outline of the process using NLP techniques:\n",
        "\n",
        "1. Data Collection and Understanding\n",
        "\n",
        "Gather customer reviews from multiple sources such as mobile apps, emails, social media, or survey forms.\n",
        "\n",
        "Inspect data for structure (JSON, CSV, text files) and metadata (date, rating, user ID).\n",
        "\n",
        "2. Data Cleaning and Preprocessing\n",
        "\n",
        "Remove noise: Eliminate duplicates, null values, and irrelevant text (URLs, emojis, HTML tags).\n",
        "\n",
        "Text normalization:\n",
        "\n",
        "Convert to lowercase.\n",
        "\n",
        "Remove punctuation, numbers, and stopwords.\n",
        "\n",
        "Apply tokenization to split text into words.\n",
        "\n",
        "Use lemmatization or stemming to reduce words to their base form.\n",
        "\n",
        "Handle special cases: Detect and manage slang, abbreviations, or domain-specific terms (e.g., “KYC”, “OTP”).\n",
        "\n",
        "3. Exploratory Text Analysis\n",
        "\n",
        "Generate word frequency distributions and word clouds to identify common topics.\n",
        "\n",
        "Perform n-gram analysis (bigrams, trigrams) to detect recurring phrases like “loan approval” or “account issue.”\n",
        "\n",
        "4. Sentiment Analysis\n",
        "\n",
        "Use pretrained models (like VADER or TextBlob) to classify reviews as positive, negative, or neutral.\n",
        "\n",
        "Aggregate sentiment scores by product, service type, or time period to identify trends.\n",
        "\n",
        "5. Topic Modeling\n",
        "\n",
        "Apply Latent Dirichlet Allocation (LDA) or BERTopic to uncover key themes (e.g., “customer support,” “transaction delays”).\n",
        "\n",
        "Label each review by dominant topic for managerial insights.\n",
        "\n",
        "6. Named Entity Recognition (NER)\n",
        "\n",
        "Use spaCy or transformers to identify named entities like product names, competitors, or transaction types.\n",
        "\n",
        "7. Visualization and Reporting\n",
        "\n",
        "Visualize sentiment trends over time, word frequencies, and topic distributions.\n",
        "\n",
        "Prepare actionable reports highlighting major customer concerns, satisfaction drivers, and improvement areas.\n",
        "\n",
        "8. Continuous Monitoring\n",
        "\n",
        "Automate pipelines to process new feedback in real time, integrating dashboards with tools like Streamlit or Power BI for ongoing insight delivery.\n",
        "\n",
        "This structured NLP workflow turns raw customer reviews into strategic intelligence, helping fintech companies enhance products, improve user experience, and boost customer retention."
      ],
      "metadata": {
        "id": "zqQ0vDW_Z751"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Om5zkfnaZ6op"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}