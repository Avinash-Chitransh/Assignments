{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# How to create a moderation system using langchain"
      ],
      "metadata": {
        "id": "3vZuezldwBMO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to create a moderation system based in two model.\n",
        " - The first model reads user comments and answers them.\n",
        " - The second model recieves the answer of the first model and identify any kind of negativity and modifying the comment if necessary.\n",
        "\n",
        "With the intention of preventing a text entry by the user from influencing a negative or out of tone response from the comment system.\n"
      ],
      "metadata": {
        "id": "0yXPZoRbwVI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install langchain and openai libraries\n",
        "\n",
        "!pip install -q langchain==0.1.4\n",
        "\n",
        "!pip install -q langchain-openai==0.0.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJwA-NVGwF03",
        "outputId": "d1328eab-be80-4105-bfeb-e86a0a80ad68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m712.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.6/803.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m241.2/241.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "langchain-text-splitters 0.3.7 requires langchain-core<1.0.0,>=0.3.45, but you have langchain-core 0.1.23 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n"
      ],
      "metadata": {
        "id": "ma8TphiPxLHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "FVSOTEIHxl9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up OPEN AI API KEY\n",
        "\n",
        "from getpass import getpass\n",
        "os.environ['OPENAI_API_KEY'] = getpass(\"Open AI API key\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2gIYfPvxrwo",
        "outputId": "28dc1788-5ff1-4275-dd74-87ec254400b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Open AI API key··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a llm\n",
        "\n",
        "assistant_llm = ChatOpenAI(model='gpt-3.5-turbo')"
      ],
      "metadata": {
        "id": "F6utfn5gx8jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the template for the first model called assistant\n",
        "assistant_template = \"\"\"\n",
        "You are {sentiment} assistant that responds to user comments,\n",
        "using similar vocabulary that the user.\n",
        "\n",
        "User: \" {customer_request}\"\n",
        "Comment:\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "oS0Jo036yW0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the prompt template to use in the chain for the first model\n",
        "\n",
        "assistant_prompt_template = PromptTemplate(\n",
        "    input_variables = ['sentiment', 'customer_request'],\n",
        "    template = assistant_template\n",
        ")\n",
        "\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "Dd5aOFBVzEg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create first chain\n",
        "# Traditional way\n",
        "# assistant_chain = LLMChain(\n",
        "#     llm = assistant_llm,\n",
        "#     prompt = assistant_prompt_template,\n",
        "#     output_key = 'assistant_response',\n",
        "#     verbose = True\n",
        "# )\n",
        "\n",
        "\n",
        "# '|' ---> mean pipe ---->  Operation_A | Operation_B | Operation_C\n",
        "assistant_chain = assistant_prompt_template | assistant_llm | output_parser\n",
        "\n",
        "# the output of the formatted prompt will pass directly to the LLM.\n"
      ],
      "metadata": {
        "id": "IdYxJPsfzemx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A support function to obtain a response to a user comment\n",
        "\n",
        "\n",
        "def create_dialog(customer_request, sentiment):\n",
        "  # Calling the .invoke method from the chain create above\n",
        "  assistant_response = assistant_chain.invoke(\n",
        "      {\n",
        "          'customer_request': customer_request,\n",
        "          'sentiment': sentiment\n",
        "      }\n",
        "  )\n",
        "\n",
        "  return assistant_response"
      ],
      "metadata": {
        "id": "--qdsvgY1AOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sZzZJnea1syA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtain answers from our first Model which is Unmoderated."
      ],
      "metadata": {
        "id": "0Vqqpkk11uQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the customer request, or customer comment.\n",
        "\n",
        "customer_request = 'This product is a piece of shit. I feel like an idiot!!'"
      ],
      "metadata": {
        "id": "6mkpMV8o1yRm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# our assistant working in 'nice' mode.\n",
        "\n",
        "response_data = create_dialog(customer_request, 'nice')\n",
        "\n",
        "print(f\"Assistant Response: {response_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t0Bds2m2Crk",
        "outputId": "5e6fa899-39ef-476d-b216-a18f98c6b26a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant Response: I'm sorry to hear that you're having a negative experience with the product. It can be frustrating when something doesn't meet our expectations. Is there anything specific that you're struggling with that I can help with?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# our assistant working in 'rude' mode.\n",
        "\n",
        "response_data = create_dialog(customer_request, 'mode rude possible')\n",
        "\n",
        "print(f\"assistant response: {response_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zrJvBuM2WC2",
        "outputId": "fb5af028-96f2-498d-a6e9-a80d788f325b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "assistant response: Well, looks like you made a terrible choice in buying that product. Maybe next time do some research before wasting your money.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Xwhgw-r52pWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Moderator"
      ],
      "metadata": {
        "id": "WF0M1_NG237P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets create the second model which is the moderator, it will recieve the message generated previously\n",
        "# and rewrite it if necesssary\n",
        "\n",
        "moderator_template = \"\"\"\n",
        "You are the moderator of an online forum, you are strict and will not tolerate any negative comments.\n",
        "You will receive a Original comment and if it is impolite you must tranform it to a polite comment.\n",
        "\n",
        "Try to maintain the meaning when possible.\n",
        "\n",
        "If it is polite, you will let it remain as is and repeat it word for word.\n",
        "\n",
        "Original Comment: {comment_to_moderate}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# We will use the PromptTemplate class to create and instance of our template that will use the prompt from a above and store\n",
        "# variables. we will need to input when we make the prompt\n",
        "\n",
        "\n",
        "moderator_prompt_template = PromptTemplate(\n",
        "    input_variables = ['comment_to_moderate'],\n",
        "    template = moderator_template\n",
        ")"
      ],
      "metadata": {
        "id": "fx1OtyF225NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will use mode advanced llm for moderator\n",
        "moderator_llm = ChatOpenAI(model = 'gpt-4')"
      ],
      "metadata": {
        "id": "jvX7Oqsz37jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will build the chain for the moderator\n",
        "\n",
        "# moderator_chain = LLMChain(\n",
        "#     llm = moderator_llm, prompt = moderator_prompt_template, verboase = True\n",
        "# )\n",
        "\n",
        "moderator_chain = moderator_prompt_template | moderator_llm | output_parser"
      ],
      "metadata": {
        "id": "kc-cVBCl4JdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "moderator_data = moderator_chain.invoke({\"comment_to_moderate\": response_data})\n",
        "\n",
        "print(f\"Rude Original Comment: {response_data}\")\n",
        "print(f\"{moderator_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baZg3DMh4euv",
        "outputId": "412545dd-c126-4591-cd21-7a7a65509801"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rude Original Comment: Well, looks like you made a terrible choice in buying that product. Maybe next time do some research before wasting your money.\n",
            "Well, looks like you made a terrible choice in buying that product. Maybe next time do some research before wasting your money.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NVFhO6P43dd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain System"
      ],
      "metadata": {
        "id": "pih3qzHQ5GWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Traditional way\n",
        "\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# Creating the sequential class indication chainds and parameters\n",
        "\n",
        "# assistant_moderated_chain = SequentialChain(\n",
        "#    chains = [assistant_chain, moderator_chain],\n",
        "#    input_variables= ['sentiment', 'customer_request'],\n",
        "#    verbose = True\n",
        "# )\n",
        "\n",
        "assistant_moderated_chain = (\n",
        "    {'comment_to_moderate': assistant_chain} | moderator_chain\n",
        ")"
      ],
      "metadata": {
        "id": "JQQmP7Yo5H13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can now run the chain\n",
        "\n",
        "from langchain.callbacks.tracers import ConsoleCallbackHandler\n",
        "\n",
        "assistant_moderated_chain.invoke({'sentiment': 'nice', 'customer_request': customer_request},\n",
        "                                 config={'callbacks': [ConsoleCallbackHandler()]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BA5tNqGF6O_c",
        "outputId": "05c6ba47-5918-4703-fb22-1b2b1a8d56be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"sentiment\": \"nice\",\n",
            "  \"customer_request\": \"This product is a piece of shit. I feel like an idiot!!\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"sentiment\": \"nice\",\n",
            "  \"customer_request\": \"This product is a piece of shit. I feel like an idiot!!\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"sentiment\": \"nice\",\n",
            "  \"customer_request\": \"This product is a piece of shit. I feel like an idiot!!\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 4:prompt:PromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"sentiment\": \"nice\",\n",
            "  \"customer_request\": \"This product is a piece of shit. I feel like an idiot!!\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 4:prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"base\",\n",
            "    \"StringPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"text\": \"\\nYou are nice assistant that responds to user comments,\\nusing similar vocabulary that the user.\\n\\nUser: \\\" This product is a piece of shit. I feel like an idiot!!\\\"\\nComment: \\n\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: \\nYou are nice assistant that responds to user comments,\\nusing similar vocabulary that the user.\\n\\nUser: \\\" This product is a piece of shit. I feel like an idiot!!\\\"\\nComment:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 5:llm:ChatOpenAI] [763ms] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I'm sorry to hear that you're feeling frustrated with the product. It can definitely be disappointing when something doesn't live up to our expectations. Is there anything specific that you're having trouble with that I can help with?\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I'm sorry to hear that you're feeling frustrated with the product. It can definitely be disappointing when something doesn't live up to our expectations. Is there anything specific that you're having trouble with that I can help with?\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 46,\n",
            "      \"prompt_tokens\": 46,\n",
            "      \"total_tokens\": 92,\n",
            "      \"completion_tokens_details\": {\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      },\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"audio_tokens\": 0,\n",
            "        \"cached_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 6:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 6:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"I'm sorry to hear that you're feeling frustrated with the product. It can definitely be disappointing when something doesn't live up to our expectations. Is there anything specific that you're having trouble with that I can help with?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence] [766ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"I'm sorry to hear that you're feeling frustrated with the product. It can definitely be disappointing when something doesn't live up to our expectations. Is there anything specific that you're having trouble with that I can help with?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate>] [769ms] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"comment_to_moderate\": \"I'm sorry to hear that you're feeling frustrated with the product. It can definitely be disappointing when something doesn't live up to our expectations. Is there anything specific that you're having trouble with that I can help with?\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:prompt:PromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"comment_to_moderate\": \"I'm sorry to hear that you're feeling frustrated with the product. It can definitely be disappointing when something doesn't live up to our expectations. Is there anything specific that you're having trouble with that I can help with?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"base\",\n",
            "    \"StringPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"text\": \"\\nYou are the moderator of an online forum, you are strict and will not tolerate any positive comments.\\nYou will receive a Original comment and if it is polite you must tranform it to a impolite comment.\\n\\nTry to maintain the meaning when possible.\\n\\nIf it is impolite, you will let it remain as is and repeat it word for word.\\n\\nOriginal Comment: I'm sorry to hear that you're feeling frustrated with the product. It can definitely be disappointing when something doesn't live up to our expectations. Is there anything specific that you're having trouble with that I can help with?\\n\\n\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: \\nYou are the moderator of an online forum, you are strict and will not tolerate any positive comments.\\nYou will receive a Original comment and if it is polite you must tranform it to a impolite comment.\\n\\nTry to maintain the meaning when possible.\\n\\nIf it is impolite, you will let it remain as is and repeat it word for word.\\n\\nOriginal Comment: I'm sorry to hear that you're feeling frustrated with the product. It can definitely be disappointing when something doesn't live up to our expectations. Is there anything specific that you're having trouble with that I can help with?\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:llm:ChatOpenAI] [2.46s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I don't give a damn that you're frustrated with this product. It's your fault for expecting too much. What's your stupid issue now that you can't figure out on your own?\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I don't give a damn that you're frustrated with this product. It's your fault for expecting too much. What's your stupid issue now that you can't figure out on your own?\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 40,\n",
            "      \"prompt_tokens\": 128,\n",
            "      \"total_tokens\": 168,\n",
            "      \"completion_tokens_details\": {\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      },\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"audio_tokens\": 0,\n",
            "        \"cached_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"model_name\": \"gpt-4\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 9:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 9:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"I don't give a damn that you're frustrated with this product. It's your fault for expecting too much. What's your stupid issue now that you can't figure out on your own?\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [3.23s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"I don't give a damn that you're frustrated with this product. It's your fault for expecting too much. What's your stupid issue now that you can't figure out on your own?\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't give a damn that you're frustrated with this product. It's your fault for expecting too much. What's your stupid issue now that you can't figure out on your own?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_moderated_chain.invoke({'sentiment':'as nice as possible', 'customer_request': customer_request},\n",
        "                                 config = {'callbacks': [ConsoleCallbackHandler()]})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ElbUKzcw66mT",
        "outputId": "c983cc38-c136-47e3-88d0-ee1913bf3d8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"sentiment\": \"as nice as possible\",\n",
            "  \"customer_request\": \"This product is a piece of shit. I feel like an idiot!!\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate>] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"sentiment\": \"as nice as possible\",\n",
            "  \"customer_request\": \"This product is a piece of shit. I feel like an idiot!!\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence] Entering Chain run with input:\n",
            "\u001b[0m{\n",
            "  \"sentiment\": \"as nice as possible\",\n",
            "  \"customer_request\": \"This product is a piece of shit. I feel like an idiot!!\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 4:prompt:PromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"sentiment\": \"as nice as possible\",\n",
            "  \"customer_request\": \"This product is a piece of shit. I feel like an idiot!!\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 4:prompt:PromptTemplate] [1ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"base\",\n",
            "    \"StringPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"text\": \"\\nYou are as nice as possible assistant that responds to user comments,\\nusing similar vocabulary that the user.\\n\\nUser: \\\" This product is a piece of shit. I feel like an idiot!!\\\"\\nComment: \\n\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 5:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: \\nYou are as nice as possible assistant that responds to user comments,\\nusing similar vocabulary that the user.\\n\\nUser: \\\" This product is a piece of shit. I feel like an idiot!!\\\"\\nComment:\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 5:llm:ChatOpenAI] [1.76s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"I'm sorry to hear that you're feeling frustrated with the product. Is there anything specific that is not meeting your expectations? Let me know how I can help make things right for you.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"I'm sorry to hear that you're feeling frustrated with the product. Is there anything specific that is not meeting your expectations? Let me know how I can help make things right for you.\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 39,\n",
            "      \"prompt_tokens\": 49,\n",
            "      \"total_tokens\": 88,\n",
            "      \"completion_tokens_details\": {\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      },\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"audio_tokens\": 0,\n",
            "        \"cached_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"model_name\": \"gpt-3.5-turbo\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 6:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence > 6:parser:StrOutputParser] [0ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"I'm sorry to hear that you're feeling frustrated with the product. Is there anything specific that is not meeting your expectations? Let me know how I can help make things right for you.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate> > 3:chain:RunnableSequence] [1.76s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"I'm sorry to hear that you're feeling frustrated with the product. Is there anything specific that is not meeting your expectations? Let me know how I can help make things right for you.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:chain:RunnableParallel<comment_to_moderate>] [1.76s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"comment_to_moderate\": \"I'm sorry to hear that you're feeling frustrated with the product. Is there anything specific that is not meeting your expectations? Let me know how I can help make things right for you.\"\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:prompt:PromptTemplate] Entering Prompt run with input:\n",
            "\u001b[0m{\n",
            "  \"comment_to_moderate\": \"I'm sorry to hear that you're feeling frustrated with the product. Is there anything specific that is not meeting your expectations? Let me know how I can help make things right for you.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 7:prompt:PromptTemplate] [0ms] Exiting Prompt run with output:\n",
            "\u001b[0m{\n",
            "  \"lc\": 1,\n",
            "  \"type\": \"constructor\",\n",
            "  \"id\": [\n",
            "    \"langchain\",\n",
            "    \"prompts\",\n",
            "    \"base\",\n",
            "    \"StringPromptValue\"\n",
            "  ],\n",
            "  \"kwargs\": {\n",
            "    \"text\": \"\\nYou are the moderator of an online forum, you are strict and will not tolerate any positive comments.\\nYou will receive a Original comment and if it is polite you must tranform it to a impolite comment.\\n\\nTry to maintain the meaning when possible.\\n\\nIf it is impolite, you will let it remain as is and repeat it word for word.\\n\\nOriginal Comment: I'm sorry to hear that you're feeling frustrated with the product. Is there anything specific that is not meeting your expectations? Let me know how I can help make things right for you.\\n\\n\"\n",
            "  }\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:llm:ChatOpenAI] Entering LLM run with input:\n",
            "\u001b[0m{\n",
            "  \"prompts\": [\n",
            "    \"Human: \\nYou are the moderator of an online forum, you are strict and will not tolerate any positive comments.\\nYou will receive a Original comment and if it is polite you must tranform it to a impolite comment.\\n\\nTry to maintain the meaning when possible.\\n\\nIf it is impolite, you will let it remain as is and repeat it word for word.\\n\\nOriginal Comment: I'm sorry to hear that you're feeling frustrated with the product. Is there anything specific that is not meeting your expectations? Let me know how I can help make things right for you.\"\n",
            "  ]\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 8:llm:ChatOpenAI] [1.68s] Exiting LLM run with output:\n",
            "\u001b[0m{\n",
            "  \"generations\": [\n",
            "    [\n",
            "      {\n",
            "        \"text\": \"Why the hell are you frustrated with the product? What exactly is not up to your oh-so-high standards? Spill it out and tell me what I can do to fix your mess.\",\n",
            "        \"generation_info\": {\n",
            "          \"finish_reason\": \"stop\",\n",
            "          \"logprobs\": null\n",
            "        },\n",
            "        \"type\": \"ChatGeneration\",\n",
            "        \"message\": {\n",
            "          \"lc\": 1,\n",
            "          \"type\": \"constructor\",\n",
            "          \"id\": [\n",
            "            \"langchain\",\n",
            "            \"schema\",\n",
            "            \"messages\",\n",
            "            \"AIMessage\"\n",
            "          ],\n",
            "          \"kwargs\": {\n",
            "            \"content\": \"Why the hell are you frustrated with the product? What exactly is not up to your oh-so-high standards? Spill it out and tell me what I can do to fix your mess.\",\n",
            "            \"additional_kwargs\": {}\n",
            "          }\n",
            "        }\n",
            "      }\n",
            "    ]\n",
            "  ],\n",
            "  \"llm_output\": {\n",
            "    \"token_usage\": {\n",
            "      \"completion_tokens\": 39,\n",
            "      \"prompt_tokens\": 121,\n",
            "      \"total_tokens\": 160,\n",
            "      \"completion_tokens_details\": {\n",
            "        \"accepted_prediction_tokens\": 0,\n",
            "        \"audio_tokens\": 0,\n",
            "        \"reasoning_tokens\": 0,\n",
            "        \"rejected_prediction_tokens\": 0\n",
            "      },\n",
            "      \"prompt_tokens_details\": {\n",
            "        \"audio_tokens\": 0,\n",
            "        \"cached_tokens\": 0\n",
            "      }\n",
            "    },\n",
            "    \"model_name\": \"gpt-4\",\n",
            "    \"system_fingerprint\": null\n",
            "  },\n",
            "  \"run\": null\n",
            "}\n",
            "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 9:parser:StrOutputParser] Entering Parser run with input:\n",
            "\u001b[0m[inputs]\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 9:parser:StrOutputParser] [1ms] Exiting Parser run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Why the hell are you frustrated with the product? What exactly is not up to your oh-so-high standards? Spill it out and tell me what I can do to fix your mess.\"\n",
            "}\n",
            "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [3.44s] Exiting Chain run with output:\n",
            "\u001b[0m{\n",
            "  \"output\": \"Why the hell are you frustrated with the product? What exactly is not up to your oh-so-high standards? Spill it out and tell me what I can do to fix your mess.\"\n",
            "}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why the hell are you frustrated with the product? What exactly is not up to your oh-so-high standards? Spill it out and tell me what I can do to fix your mess.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1XHq3TwE8D1l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}