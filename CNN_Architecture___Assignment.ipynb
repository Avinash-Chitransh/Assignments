{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.  What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        "\n",
        "a. Filters (Kernels)\n",
        "\n",
        "A filter is a small matrix of weights (e.g., 3√ó3, 5√ó5) that slides across the input image or previous feature map.\n",
        "\n",
        "Each filter extracts specific local features such as edges, corners, textures, or more complex shapes as the network goes deeper.\n",
        "\n",
        "The values in the filter are learned automatically during training via backpropagation, so the network discovers the most useful patterns.\n",
        "\n",
        "Different filters focus on different features: one may detect horizontal edges, another vertical lines, another curves, etc.\n",
        "\n",
        "Mathematically, the operation is a convolution:\n",
        "\n",
        "Feature¬†Map =\n",
        "Input\n",
        "‚àó\n",
        "Filter\n",
        "+\n",
        "Bias\n",
        "\n",
        "b. Feature Maps\n",
        "\n",
        "A feature map (also called an activation map) is the output of applying a filter to the input.\n",
        "\n",
        "It highlights where a certain feature (like an edge or shape) is present in the input.\n",
        "\n",
        "Stacking multiple filters produces multiple feature maps, each capturing a different type of feature.\n",
        "\n",
        "As we go deeper into the CNN, feature maps capture progressively more abstract patterns:\n",
        "\n",
        "Early layers ‚Üí low-level features (edges, colors, textures).\n",
        "\n",
        "Middle layers ‚Üí shapes, corners, object parts.\n",
        "\n",
        "Deep layers ‚Üí high-level semantic features (e.g., faces, wheels, digits)."
      ],
      "metadata": {
        "id": "bUZcMY2oEA_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "In a Convolutional Neural Network (CNN), padding and stride are two key parameters that directly influence the size of the output feature maps. Padding refers to the practice of adding extra rows and columns, usually filled with zeros, around the input image before applying a convolution. Its primary purpose is to control how much of the border information is preserved during convolution and to prevent the feature maps from shrinking too quickly. Without padding (called valid padding), the output becomes smaller than the input after each convolution. With zero padding (often called same padding), enough zeros are added so that the output feature map has the same spatial dimensions as the input when the stride is one.\n",
        "\n",
        "Stride, on the other hand, determines how many steps the convolutional filter moves across the input. A stride of one means the filter slides one pixel at a time, creating the maximum number of overlapping regions and thus producing a larger output feature map. Increasing the stride reduces overlap, making the output smaller and effectively downsampling the input. The relationship between input size, filter size, padding, and stride can be expressed mathematically as:\n",
        "\n",
        "Output¬†Size =\n",
        "ùëä\n",
        "‚àí\n",
        "ùêπ\n",
        "+\n",
        "2\n",
        "ùëÉ\n",
        "ùëÜ\n",
        "+\n",
        "1\n",
        "\n",
        "where\n",
        "\n",
        "W is the input dimension,\n",
        "\n",
        "F is the filter size,\n",
        "\n",
        "P is the padding, and\n",
        "\n",
        "S is the stride.\n",
        "\n",
        "For example, a 7√ó7 input with a 3√ó3 filter, stride of 2, and padding of 1 produces a 4√ó4 output feature map. In summary, padding ensures that edge information is not lost and controls spatial size, while stride regulates the degree of downsampling. Together, they determine the resolution and scale of the feature maps in a CNN."
      ],
      "metadata": {
        "id": "6sTRHRXrEw7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?\n",
        "\n",
        "In the context of Convolutional Neural Networks (CNNs), the receptive field refers to the specific region of the input image that influences a particular neuron or unit in a feature map. In simpler terms, it is the portion of the input data that a neuron ‚Äúsees‚Äù or is sensitive to. For the first convolutional layer, the receptive field is just the size of the filter (for example, a 3√ó3 kernel). However, as we move deeper into the network, the receptive field of neurons increases because each layer combines information from multiple neurons of the previous layer. Eventually, deeper neurons capture patterns that span larger areas of the original image.\n",
        "\n",
        "The receptive field is important in deep architectures because it determines how much contextual information the network can capture. In shallow layers, a small receptive field allows the model to focus on fine-grained local features such as edges, corners, or textures. As the network grows deeper, the receptive field expands, enabling neurons to integrate these local patterns into more complex structures, such as object parts or even entire objects. If the receptive field is too small, the model may fail to capture global context, which is essential for tasks like image classification, object detection, and scene understanding. Conversely, a sufficiently large receptive field ensures that the CNN can effectively recognize both local details and global patterns, making deep architectures powerful for handling complex visual data."
      ],
      "metadata": {
        "id": "ynoXtCPRFSoS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Discuss how filter size and stride influence the number of parameters in a CNN.\n",
        "\n",
        "In a Convolutional Neural Network (CNN), the filter size and stride affect the architecture in different ways, particularly in terms of the number of parameters and the spatial resolution of feature maps.\n",
        "\n",
        "a. Effect of Filter Size on Parameters\n",
        "\n",
        "The number of parameters in a convolutional layer depends on the filter size, the depth of the input, and the number of filters. For a filter of size\n",
        "\n",
        "F√óF applied to an input with depth\n",
        "\n",
        "D, the parameters per filter are:\n",
        "\n",
        "Parameters¬†per¬†filter =\n",
        "(\n",
        "ùêπ\n",
        "√ó\n",
        "ùêπ\n",
        "√ó\n",
        "ùê∑\n",
        ")\n",
        "+\n",
        "1\n",
        "\n",
        "(The +1 accounts for the bias term.)\n",
        "\n",
        "Thus, larger filters contain more parameters because each filter must learn more weights. For example, with a\n",
        "\n",
        "3√ó3 filter on RGB images (D=3), one filter has (3√ó3√ó3)+1= 28\n",
        "\n",
        "parameters, while a 5√ó5 filter has\n",
        "(5√ó5√ó3)+1=76 parameters. Importantly, the filter size directly controls model complexity, with larger filters learning broader spatial features but at a higher computational cost.\n",
        "\n",
        "2. Effect of Stride on Parameters\n",
        "\n",
        "Unlike filter size, the stride does not change the number of parameters, because parameters depend only on the filter dimensions and input depth. Stride simply controls how far the filter moves across the input. A larger stride reduces the number of positions where the filter is applied, which decreases the number of activations (output size of the feature map) but not the learned weights.\n",
        "\n",
        "For example, with a stride of 1, a\n",
        "\n",
        "3√ó3 filter slides across nearly every pixel, producing a dense feature map. With a stride of 2, the same filter skips every other pixel, producing a smaller feature map. The parameter count remains the same, but the computational cost and memory usage decrease because fewer activations are computed.\n",
        "\n",
        "Summary\n",
        "\n",
        "Filter size directly affects the number of parameters: larger filters = more weights to learn.\n",
        "\n",
        "Stride does not change the number of parameters but influences how many activations are produced, thus impacting computational efficiency and the degree of downsampling."
      ],
      "metadata": {
        "id": "G9mCMkfWFfq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "CNN architectures have evolved significantly over time, and LeNet, AlexNet, and VGG are milestones that illustrate this progression. They differ in depth, filter design, and overall performance.\n",
        "\n",
        "a. LeNet (1990s)\n",
        "\n",
        "Depth: One of the earliest CNNs, designed by Yann LeCun for digit recognition (MNIST). It had about 5‚Äì7 layers including convolutional and fully connected layers.\n",
        "\n",
        "Filter sizes: Used small filters, typically\n",
        "\n",
        "5√ó5. Pooling was average pooling, not max pooling.\n",
        "\n",
        "Performance: Worked well for small grayscale images (like handwritten digits) but was not designed for large-scale image classification. Its simplicity made it foundational but limited in scalability.\n",
        "\n",
        "b. AlexNet (2012)\n",
        "\n",
        "Depth: Much deeper than LeNet, with 8 layers (5 convolutional + 3 fully connected).\n",
        "\n",
        "Filter sizes: The first convolutional layer used a large\n",
        "\n",
        "11√ó11 filter with stride 4, followed by smaller\n",
        "\n",
        "5√ó5 and\n",
        "\n",
        "3√ó3 filters. Max pooling replaced average pooling.\n",
        "\n",
        "Performance: Achieved a breakthrough by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 2012, reducing error rates by nearly 10%. It introduced ReLU activations, dropout for regularization, and parallel training on GPUs.\n",
        "\n",
        "c. VGG (2014)\n",
        "\n",
        "Depth: Much deeper, with 16 to 19 layers, showcasing the power of very deep networks.\n",
        "\n",
        "Filter sizes: Standardized on small\n",
        "\n",
        "3√ó3 filters stacked together to effectively simulate larger receptive fields (e.g., two\n",
        "\n",
        "3√ó3 filters ‚âà one\n",
        "\n",
        "5√ó5). Stride and pooling were uniform.\n",
        "\n",
        "Performance: Achieved high accuracy on ImageNet with a simple and elegant design, proving that depth with uniform filters can greatly improve performance. However, it required massive computation and memory, making it expensive compared to AlexNet."
      ],
      "metadata": {
        "id": "6hH1RfZaGt9Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation."
      ],
      "metadata": {
        "id": "Do0IgVWAHN7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsMLwabArLUI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-XyC_mcHh2D",
        "outputId": "66ce9928-5d6d-4eb3-edd8-e87224788963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape((x_test.shape[0], 28, 28, 1)).astype(\"float32\") / 255.0"
      ],
      "metadata": {
        "id": "dRmIlu3WHs1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation=\"relu\"),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")   # 10 classes for digits 0‚Äì9\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKV0r1hjHwNX",
        "outputId": "f09b8a00-26e3-4322-f006-2c09a3113fa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile the model\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "R627HkwhHzvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=5,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCNRdgfZH9N7",
        "outputId": "7bf74f58-bb83-4c38-f1ab-4e12323f4a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "844/844 - 41s - 49ms/step - accuracy: 0.9498 - loss: 0.1696 - val_accuracy: 0.9843 - val_loss: 0.0520\n",
            "Epoch 2/5\n",
            "844/844 - 40s - 48ms/step - accuracy: 0.9850 - loss: 0.0503 - val_accuracy: 0.9867 - val_loss: 0.0460\n",
            "Epoch 3/5\n",
            "844/844 - 41s - 48ms/step - accuracy: 0.9896 - loss: 0.0345 - val_accuracy: 0.9893 - val_loss: 0.0378\n",
            "Epoch 4/5\n",
            "844/844 - 40s - 47ms/step - accuracy: 0.9919 - loss: 0.0263 - val_accuracy: 0.9885 - val_loss: 0.0384\n",
            "Epoch 5/5\n",
            "844/844 - 39s - 46ms/step - accuracy: 0.9933 - loss: 0.0204 - val_accuracy: 0.9908 - val_loss: 0.0323\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPi0KgSCIA_Q",
        "outputId": "bcf0a592-225a-4ed4-874c-4086e21cb2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 2s - 7ms/step - accuracy: 0.9914 - loss: 0.0267\n",
            "Test Accuracy: 0.9914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture."
      ],
      "metadata": {
        "id": "WBVgBHP4J87I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "uIFSZYS1I31W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to [0, 1]\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# Convert labels from integers to one-hot encoded vectors\n",
        "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
        "\n",
        "print(\"Training data shape:\", x_train.shape)\n",
        "print(\"Test data shape:\", x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6eeHpuLKBqF",
        "outputId": "98cbd4bf-f2f9-4f31-f601-9dd3ba38481f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Training data shape: (50000, 32, 32, 3)\n",
            "Test data shape: (10000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    # First convolutional block\n",
        "    layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\", input_shape=(32, 32, 3)),\n",
        "    layers.Conv2D(32, (3,3), activation=\"relu\", padding=\"same\"),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Second convolutional block\n",
        "    layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\"),\n",
        "    layers.Conv2D(64, (3,3), activation=\"relu\", padding=\"same\"),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Dropout(0.25),\n",
        "\n",
        "    # Flatten and dense layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation=\"softmax\")  # 10 classes\n",
        "])"
      ],
      "metadata": {
        "id": "Uhce4Pi3KEf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93ca9ad1-ef2f-477d-d167-36693aab1ff8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "metadata": {
        "id": "A-wK7VwtKXD2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    x_train, y_train,\n",
        "    epochs=10,\n",
        "    batch_size=64,\n",
        "    validation_split=0.1,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O01FOKzgKZed",
        "outputId": "98022567-42df-44a7-e662-78a4a2d9d1f4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "704/704 - 17s - 24ms/step - accuracy: 0.4254 - loss: 1.5723 - val_accuracy: 0.5672 - val_loss: 1.2045\n",
            "Epoch 2/10\n",
            "704/704 - 5s - 6ms/step - accuracy: 0.5864 - loss: 1.1633 - val_accuracy: 0.6548 - val_loss: 0.9582\n",
            "Epoch 3/10\n",
            "704/704 - 4s - 6ms/step - accuracy: 0.6496 - loss: 0.9918 - val_accuracy: 0.7004 - val_loss: 0.8757\n",
            "Epoch 4/10\n",
            "704/704 - 5s - 7ms/step - accuracy: 0.6872 - loss: 0.8835 - val_accuracy: 0.6902 - val_loss: 0.8706\n",
            "Epoch 5/10\n",
            "704/704 - 5s - 7ms/step - accuracy: 0.7136 - loss: 0.8109 - val_accuracy: 0.7416 - val_loss: 0.7431\n",
            "Epoch 6/10\n",
            "704/704 - 5s - 7ms/step - accuracy: 0.7351 - loss: 0.7494 - val_accuracy: 0.7302 - val_loss: 0.7656\n",
            "Epoch 7/10\n",
            "704/704 - 5s - 7ms/step - accuracy: 0.7552 - loss: 0.6935 - val_accuracy: 0.7716 - val_loss: 0.6684\n",
            "Epoch 8/10\n",
            "704/704 - 5s - 7ms/step - accuracy: 0.7671 - loss: 0.6573 - val_accuracy: 0.7712 - val_loss: 0.6616\n",
            "Epoch 9/10\n",
            "704/704 - 4s - 6ms/step - accuracy: 0.7814 - loss: 0.6178 - val_accuracy: 0.7750 - val_loss: 0.6632\n",
            "Epoch 10/10\n",
            "704/704 - 5s - 7ms/step - accuracy: 0.7946 - loss: 0.5821 - val_accuracy: 0.7776 - val_loss: 0.6571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "id": "s5HdBdmtKbHq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4df3bf5-bd9c-4161-c098-4968215ca443"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 2s - 6ms/step - accuracy: 0.7670 - loss: 0.6872\n",
            "Test Accuracy: 0.7670\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['accuracy'], label=\"Train Accuracy\")\n",
        "plt.plot(history.history['val_accuracy'], label=\"Val Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "W9KNxW2CN_Za",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "045e2289-4e82-4a1c-9fcc-3c44d7638937"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation."
      ],
      "metadata": {
        "id": "_mhUasONPvHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader"
      ],
      "metadata": {
        "id": "ZoD6t6rpPouU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Device configuration\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDgN9YqNTNp5",
        "outputId": "595727ff-2fdf-4755-983f-488b6924d6bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data loading and preprocessing\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),                     # Convert to tensor\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize with MNIST mean & std\n",
        "])"
      ],
      "metadata": {
        "id": "IdxKCkpoTToA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=1000, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bhnlso-jTZHW",
        "outputId": "0f77dcf7-62d4-44e9-bd35-7db6d87892e8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:00<00:00, 16.5MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 562kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 4.54MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 9.51MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)   # (28x28 -> 28x28)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # (28x28 -> 28x28)\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Downsampling\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)   # 10 classes for digits"
      ],
      "metadata": {
        "id": "7dkfqcyiTciI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))   # (28x28 -> 14x14)\n",
        "        x = self.pool(F.relu(self.conv2(x)))   # (14x14 -> 7x7)\n",
        "        x = x.view(-1, 64 * 7 * 7)             # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ABJ8exJtTgZw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN().to(device)"
      ],
      "metadata": {
        "id": "gOMnTTnCTjHG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "OoxuTnDQTlLQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, loader, optimizer, criterion, epoch):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for batch_idx, (data, target) in enumerate(loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, target)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch}], Loss: {running_loss/len(loader):.4f}\")"
      ],
      "metadata": {
        "id": "QlFFuj2ETnmU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation loop\n",
        "def test(model, loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            outputs = model(data)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "T_ks1sCDTqYf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model."
      ],
      "metadata": {
        "id": "goQkd6ohvX3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Root dataset path\n",
        "dataset_path = \"custom_dataset\"\n",
        "classes = [\"cats\", \"dogs\"]\n",
        "\n",
        "# Create folders (train/val structure)\n",
        "for split in [\"train\", \"val\"]:\n",
        "    for cls in classes:\n",
        "        os.makedirs(os.path.join(dataset_path, split, cls), exist_ok=True)\n",
        "\n",
        "# Generate random images (50 train, 10 val per class)\n",
        "for cls in classes:\n",
        "    for i in range(50):\n",
        "        arr = np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8)  # random RGB image\n",
        "        img = Image.fromarray(arr)\n",
        "        img.save(os.path.join(dataset_path, \"train\", cls, f\"{cls}_{i}.jpg\"))\n",
        "    for i in range(10):\n",
        "        arr = np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8)\n",
        "        img = Image.fromarray(arr)\n",
        "        img.save(os.path.join(dataset_path, \"val\", cls, f\"{cls}_{i}.jpg\"))"
      ],
      "metadata": {
        "id": "dsktq1oZv_Io"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Paths\n",
        "train_dir = \"custom_dataset/train\"\n",
        "val_dir   = \"custom_dataset/val\"\n",
        "\n",
        "# ImageDataGenerator\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,          # normalize\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "fPKGKdiFxXqw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flow from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=16,\n",
        "    class_mode='binary'  # since 2 classes\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(64, 64),\n",
        "    batch_size=16,\n",
        "    class_mode='binary'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqmT9qz_xXnY",
        "outputId": "a9888596-98e1-44a0-dbfd-4763ceff1e2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 100 images belonging to 2 classes.\n",
            "Found 20 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN model\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(64,64,3)),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D(2,2),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoHKyj6rxXjz",
        "outputId": "5889923a-f8dd-49e0-8ae7-b369396f58fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "RrOM5pZqxXfs"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=5,\n",
        "    validation_data=val_generator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ctMqhYCQxXbE",
        "outputId": "91c302ea-ace8-46be-c30a-a53be73111cf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 478ms/step - accuracy: 0.4912 - loss: 0.7345 - val_accuracy: 0.5000 - val_loss: 0.7011\n",
            "Epoch 2/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5268 - loss: 0.7174 - val_accuracy: 0.5000 - val_loss: 0.6942\n",
            "Epoch 3/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5694 - loss: 0.6910 - val_accuracy: 0.5000 - val_loss: 0.6950\n",
            "Epoch 4/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5386 - loss: 0.6929 - val_accuracy: 0.4000 - val_loss: 0.6941\n",
            "Epoch 5/5\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.4739 - loss: 0.6935 - val_accuracy: 0.5000 - val_loss: 0.6934\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "loss, acc = model.evaluate(val_generator)\n",
        "print(f\"Validation Accuracy: {acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tiYye_2hxXWy",
        "outputId": "9a570e50-1775-4e8e-a8ce-640aae385cff"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.5208 - loss: 0.6924\n",
            "Validation Accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into ‚ÄúNormal‚Äù and ‚ÄúPneumonia‚Äù categories. Describe your end-to-end approach‚Äìfrom data preparation and model training to deploying the model as a web app using Streamlit.\n",
        "\n",
        "\n",
        "a. Data & ethics\n",
        "\n",
        "Source & labeling: collect curated chest X-ray images (DICOM or PNG/JPEG) with reliable labels (radiologist consensus). Keep provenance and versioning.\n",
        "\n",
        "Privacy & compliance: de-identify images (remove metadata), follow HIPAA / GDPR as applicable. Use secure storage (encrypted S3 buckets or private storage).\n",
        "\n",
        "Bias & dataset balance: check class balance, patient overlap (no patient in both train/val/test), age/sex distribution. If limited positive cases, use augmentation carefully.\n",
        "\n",
        "Train/val/test split: split by patient ID (e.g., 70/15/15) to avoid leakage.\n",
        "\n",
        "b. Preprocessing & augmentation\n",
        "\n",
        "Input resolution: 224√ó224 or 320√ó320 (tradeoff: GPU cost vs info).\n",
        "\n",
        "Windowing: If using DICOM, apply consistent window/level; normalize intensities.\n",
        "\n",
        "Channel: replicate grayscale to 3 channels if using ImageNet backbones.\n",
        "\n",
        "Augmentation: realistic transforms only ‚Äî small rotations (¬±10¬∞), translations, horizontal flip (use clinically appropriate flips), brightness/contrast, random zoom. Avoid unrealistic distortions.\n",
        "\n",
        "Class imbalance: use class weights, oversampling, or focal loss.\n",
        "\n",
        "Example preprocessing pipeline (Keras ImageDataGenerator or tf.data recommended for speed). I‚Äôll use tf.keras.preprocessing.image_dataset_from_directory + tf.image ops below.\n",
        "\n",
        "c. Model: transfer learning\n",
        "\n",
        "Use a robust backbone (DenseNet121, EfficientNetB0/B1, or ResNet50). Freeze backbone initially, train top classifier, then unfreeze some layers for fine-tuning.\n",
        "d. Evaluation & validation\n",
        "\n",
        "Compute confusion matrix, sensitivity (recall for Pneumonia), specificity, AUC-ROC, PR curve.\n",
        "\n",
        "Use patient-level evaluation (aggregate predictions per patient).\n",
        "\n",
        "Cross-validation across different institutions if possible.\n",
        "\n",
        "Calibration: apply Platt scaling or isotonic calibration if probabilities are used clinically.\n",
        "\n",
        "External validation: test on an external dataset from another hospital.\n",
        "\n",
        "e. Explainability ‚Äî Grad-CAM (recommended)\n",
        "\n",
        "Provide heatmaps (Grad-CAM) to highlight regions that influenced the prediction so clinicians can inspect model focus.\n",
        "\n",
        "Brief sketch of Grad-CAM usage in Keras (used in Streamlit app later).\n",
        "\n",
        "f. Packaging for deployment\n",
        "\n",
        "Save the Keras model (model.save('models/chest_xray_densenet.h5')) or convert to TF SavedModel or TensorFlow Lite/ONNX if needed.\n",
        "\n",
        "Containerize with Docker. Include GPU image if inference requires GPU.\n",
        "\n",
        "g. Deployment options\n",
        "\n",
        "Streamlit Cloud (quick): push repo, set model file to remote storage or include model (large models better in S3).\n",
        "\n",
        "Docker + Kubernetes / EKS / GCP Cloud Run: for scale and managed infra. Use GPU nodes if low latency and heavy load.\n",
        "\n",
        "Serve model separately (recommended for production): deploy model behind a REST API (FastAPI or TensorFlow Serving), and have Streamlit call that API. This separates concerns and scales inference independently.\n",
        "\n",
        "Example architecture:\n",
        "\n",
        "Storage (S3) for images + model artifacts\n",
        "\n",
        "Model server (TF Serving or FastAPI) behind load balancer\n",
        "\n",
        "Streamlit or React frontend calling model server\n",
        "\n",
        "Database for logs and audit (Postgres), monitoring stack (Prometheus/Grafana)\n",
        "\n",
        "h. Testing, monitoring & governance\n",
        "\n",
        "Unit & integration tests: image preprocessing, model outputs, API endpoints.\n",
        "\n",
        "Monitoring: track input distribution, latency, prediction distribution, model drift. Alert if sensitivity drops.\n",
        "\n",
        "Auditing & logging: log predictions, timestamps, patient hash (not PII), model version.\n",
        "\n",
        "Retraining: data pipeline to collect labeled feedback, schedule periodic retraining, and use Canary/A-B for new models.\n",
        "\n",
        "Clinical validation & approval: for any clinical deployment, conduct prospective trials and obtain regulatory approvals (FDA, CE) ‚Äî this is essential."
      ],
      "metadata": {
        "id": "o57je3znwMlP"
      }
    }
  ]
}