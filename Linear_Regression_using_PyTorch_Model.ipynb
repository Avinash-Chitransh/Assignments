{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VhcArF4C7jFw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "3hlbwOgL73G0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making training data\n",
        "# Input ---> (temp, rainfall, humidity) ---> yield of apple and oranges crops\n",
        "\n",
        "inputs = np.array([\n",
        "    [73, 67,43],\n",
        "    [91, 88, 64],\n",
        "    [87, 134, 58],\n",
        "    [102, 43, 37],\n",
        "    [69, 96, 70],\n",
        "], dtype = 'float32')"
      ],
      "metadata": {
        "id": "KA07_zd479VO"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Target (apples, oranges)\n",
        "\n",
        "target = np.array([\n",
        "    [56, 70],\n",
        "    [81, 101],\n",
        "    [119, 113],\n",
        "    [22, 37],\n",
        "    [103, 119]\n",
        "], dtype = 'float32')"
      ],
      "metadata": {
        "id": "Lc8PT83r8eQU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.from_numpy(inputs)\n",
        "target = torch.from_numpy(target)"
      ],
      "metadata": {
        "id": "tmOfQQv88hGV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)\n",
        "print(target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ke3SeZpj81mm",
        "outputId": "c0b8ec0c-dc3e-490e-a005-6217892c6b13"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 73.,  67.,  43.],\n",
            "        [ 91.,  88.,  64.],\n",
            "        [ 87., 134.,  58.],\n",
            "        [102.,  43.,  37.],\n",
            "        [ 69.,  96.,  70.]])\n",
            "tensor([[ 56.,  70.],\n",
            "        [ 81., 101.],\n",
            "        [119., 113.],\n",
            "        [ 22.,  37.],\n",
            "        [103., 119.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weights and biases\n",
        "w= torch.randn(2, 3, requires_grad=True)\n",
        "b= torch.randn(2, requires_grad=True)\n",
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GssgH-J-9Csa",
        "outputId": "5f8381d8-bd6b-4270-9eb6-2e2ee09dbdda"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2405, -0.1433, -0.8525],\n",
            "        [-0.3976, -0.5843,  0.7872]], requires_grad=True)\n",
            "tensor([-1.8330,  1.9686], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define the model\n",
        "\n",
        "# Z = X * W + B\n",
        "def model(x):\n",
        "  return x @ w.t() + b"
      ],
      "metadata": {
        "id": "OG1tUna19Paq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction\n",
        "preds = model(inputs)\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrXtUgEN9UTW",
        "outputId": "054adb8e-0511-4ca5-b845-489bbe57ea2d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-30.5366, -32.3511],\n",
            "        [-47.1197, -35.2460],\n",
            "        [-49.5597, -65.2561],\n",
            "        [-15.0075, -34.5811],\n",
            "        [-58.6722, -26.4505]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss funtion we will use is MSE -> Mean squared error\n",
        "def MSE(y, y_hat):\n",
        "  diff = y - y_hat\n",
        "\n",
        "  return torch.sum(diff*diff)/diff.numel()"
      ],
      "metadata": {
        "id": "BgvCK5aM9ew5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# error\n",
        "loss = MSE(target, preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh4Oy7uQ9njZ",
        "outputId": "807c8349-d9d2-45da-bf22-e0bf224269db"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(16691.6699, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compute gradients\n",
        "loss.backward()"
      ],
      "metadata": {
        "id": "axwW9io39qrK"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(w.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbJodC0P9wAI",
        "outputId": "92c389d0-a661-4c5d-9a00-7533c852dd66"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2405, -0.1433, -0.8525],\n",
            "        [-0.3976, -0.5843,  0.7872]], requires_grad=True)\n",
            "tensor([[ -9514.1816, -11354.2695,  -6876.7065],\n",
            "        [-10543.1299, -11954.9434,  -7257.9453]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(b)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnTcb1BS91Fr",
        "outputId": "bb74b7e3-b726-4aa9-a7ae-bfc68cd41c7a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.8330,  1.9686], requires_grad=True)\n",
            "tensor([-116.3791, -126.7769])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#reset grad\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opUTvOyr98fM",
        "outputId": "c5149de1-11de-4094-d3f2-7987528ae3e9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wF8_naKW-DzN",
        "outputId": "608fabbe-7b91-4c42-db5e-5a63b0a6529b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([0., 0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust params\n",
        "\n",
        "preds = model(inputs)\n",
        "\n",
        "print(preds)\n",
        "\n",
        "\n",
        "loss = MSE(target, preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCeislMy-HAd",
        "outputId": "539fb734-4cfd-48bf-eb91-ee924d1c6a96"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-30.5366, -32.3511],\n",
            "        [-47.1197, -35.2460],\n",
            "        [-49.5597, -65.2561],\n",
            "        [-15.0075, -34.5811],\n",
            "        [-58.6722, -26.4505]], grad_fn=<AddBackward0>)\n",
            "tensor(16691.6699, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss.backward()\n",
        "\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaV9O7j_-Juf",
        "outputId": "b7a30a08-0ddd-4159-cb77-5d0d69893343"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ -9514.1816, -11354.2695,  -6876.7065],\n",
            "        [-10543.1299, -11954.9434,  -7257.9453]])\n",
            "tensor([-116.3791, -126.7769])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adjust weight & reset grad\n",
        "\n",
        "learning_rate = 1e-5\n",
        "\n",
        "with torch.no_grad():\n",
        "  w -= w.grad * 1e-5\n",
        "  b -= b.grad * 1e-5\n",
        "\n",
        "  w.grad.zero_()\n",
        "  b.grad.zero_()"
      ],
      "metadata": {
        "id": "P8EvKwLg-Mjy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(w)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5hdBcgh-Oz7",
        "outputId": "78e63f94-91ad-4a16-edb1-9d71559a8038"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.3356, -0.0298, -0.7837],\n",
            "        [-0.2921, -0.4647,  0.8598]], requires_grad=True)\n",
            "tensor([-1.8318,  1.9698], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate again\n",
        "\n",
        "preds = model(inputs)\n",
        "loss = MSE(target, preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q320XDNe-S12",
        "outputId": "d4d71be8-fde1-4924-a074-d8ce3a856c6c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(11469.3965, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training for muliple epochs\n",
        "for i in range(400):\n",
        "  preds = model(inputs)\n",
        "  loss = MSE(target, preds)\n",
        "  loss.backward()\n",
        "  with torch.no_grad():\n",
        "    w -= w.grad * 1e-5\n",
        "    b -= b.grad * 1e-5\n",
        "\n",
        "    w.grad.zero_()\n",
        "    b.grad.zero_()\n",
        "  print(f\"Epochs({i}/{100}) & Loss {loss}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMLsM3s1-V6U",
        "outputId": "57ace076-5578-4b80-bbd0-28deaaace880"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs(0/100) & Loss 11469.396484375\n",
            "Epochs(1/100) & Loss 7947.63134765625\n",
            "Epochs(2/100) & Loss 5571.845703125\n",
            "Epochs(3/100) & Loss 3968.35400390625\n",
            "Epochs(4/100) & Loss 2885.328125\n",
            "Epochs(5/100) & Loss 2153.069091796875\n",
            "Epochs(6/100) & Loss 1657.216064453125\n",
            "Epochs(7/100) & Loss 1320.703857421875\n",
            "Epochs(8/100) & Loss 1091.598388671875\n",
            "Epochs(9/100) & Loss 934.9015502929688\n",
            "Epochs(10/100) & Loss 827.0289306640625\n",
            "Epochs(11/100) & Loss 752.0863037109375\n",
            "Epochs(12/100) & Loss 699.36279296875\n",
            "Epochs(13/100) & Loss 661.6402587890625\n",
            "Epochs(14/100) & Loss 634.053466796875\n",
            "Epochs(15/100) & Loss 613.3242797851562\n",
            "Epochs(16/100) & Loss 597.2427368164062\n",
            "Epochs(17/100) & Loss 584.3194580078125\n",
            "Epochs(18/100) & Loss 573.5504150390625\n",
            "Epochs(19/100) & Loss 564.2587280273438\n",
            "Epochs(20/100) & Loss 555.9879150390625\n",
            "Epochs(21/100) & Loss 548.4296264648438\n",
            "Epochs(22/100) & Loss 541.3765869140625\n",
            "Epochs(23/100) & Loss 534.6881103515625\n",
            "Epochs(24/100) & Loss 528.2694702148438\n",
            "Epochs(25/100) & Loss 522.0562744140625\n",
            "Epochs(26/100) & Loss 516.0050048828125\n",
            "Epochs(27/100) & Loss 510.0858459472656\n",
            "Epochs(28/100) & Loss 504.2784118652344\n",
            "Epochs(29/100) & Loss 498.5692443847656\n",
            "Epochs(30/100) & Loss 492.9482421875\n",
            "Epochs(31/100) & Loss 487.4088439941406\n",
            "Epochs(32/100) & Loss 481.94586181640625\n",
            "Epochs(33/100) & Loss 476.5559997558594\n",
            "Epochs(34/100) & Loss 471.236572265625\n",
            "Epochs(35/100) & Loss 465.98541259765625\n",
            "Epochs(36/100) & Loss 460.80084228515625\n",
            "Epochs(37/100) & Loss 455.68170166015625\n",
            "Epochs(38/100) & Loss 450.6265563964844\n",
            "Epochs(39/100) & Loss 445.63458251953125\n",
            "Epochs(40/100) & Loss 440.70452880859375\n",
            "Epochs(41/100) & Loss 435.83587646484375\n",
            "Epochs(42/100) & Loss 431.02752685546875\n",
            "Epochs(43/100) & Loss 426.2787170410156\n",
            "Epochs(44/100) & Loss 421.5887145996094\n",
            "Epochs(45/100) & Loss 416.95660400390625\n",
            "Epochs(46/100) & Loss 412.3818359375\n",
            "Epochs(47/100) & Loss 407.8636779785156\n",
            "Epochs(48/100) & Loss 403.4012145996094\n",
            "Epochs(49/100) & Loss 398.9940185546875\n",
            "Epochs(50/100) & Loss 394.64111328125\n",
            "Epochs(51/100) & Loss 390.34197998046875\n",
            "Epochs(52/100) & Loss 386.0958557128906\n",
            "Epochs(53/100) & Loss 381.902099609375\n",
            "Epochs(54/100) & Loss 377.76019287109375\n",
            "Epochs(55/100) & Loss 373.6691589355469\n",
            "Epochs(56/100) & Loss 369.6287536621094\n",
            "Epochs(57/100) & Loss 365.6380615234375\n",
            "Epochs(58/100) & Loss 361.69647216796875\n",
            "Epochs(59/100) & Loss 357.8034973144531\n",
            "Epochs(60/100) & Loss 353.95831298828125\n",
            "Epochs(61/100) & Loss 350.16058349609375\n",
            "Epochs(62/100) & Loss 346.40972900390625\n",
            "Epochs(63/100) & Loss 342.7048034667969\n",
            "Epochs(64/100) & Loss 339.04559326171875\n",
            "Epochs(65/100) & Loss 335.43133544921875\n",
            "Epochs(66/100) & Loss 331.86151123046875\n",
            "Epochs(67/100) & Loss 328.33551025390625\n",
            "Epochs(68/100) & Loss 324.8528137207031\n",
            "Epochs(69/100) & Loss 321.412841796875\n",
            "Epochs(70/100) & Loss 318.0152587890625\n",
            "Epochs(71/100) & Loss 314.65936279296875\n",
            "Epochs(72/100) & Loss 311.344482421875\n",
            "Epochs(73/100) & Loss 308.07037353515625\n",
            "Epochs(74/100) & Loss 304.8364562988281\n",
            "Epochs(75/100) & Loss 301.64215087890625\n",
            "Epochs(76/100) & Loss 298.48699951171875\n",
            "Epochs(77/100) & Loss 295.3704528808594\n",
            "Epochs(78/100) & Loss 292.2922668457031\n",
            "Epochs(79/100) & Loss 289.2514953613281\n",
            "Epochs(80/100) & Loss 286.2480773925781\n",
            "Epochs(81/100) & Loss 283.2813720703125\n",
            "Epochs(82/100) & Loss 280.35107421875\n",
            "Epochs(83/100) & Loss 277.45648193359375\n",
            "Epochs(84/100) & Loss 274.59735107421875\n",
            "Epochs(85/100) & Loss 271.77313232421875\n",
            "Epochs(86/100) & Loss 268.9832763671875\n",
            "Epochs(87/100) & Loss 266.2276611328125\n",
            "Epochs(88/100) & Loss 263.50555419921875\n",
            "Epochs(89/100) & Loss 260.8167724609375\n",
            "Epochs(90/100) & Loss 258.1607971191406\n",
            "Epochs(91/100) & Loss 255.5370635986328\n",
            "Epochs(92/100) & Loss 252.9453887939453\n",
            "Epochs(93/100) & Loss 250.3853302001953\n",
            "Epochs(94/100) & Loss 247.85635375976562\n",
            "Epochs(95/100) & Loss 245.35830688476562\n",
            "Epochs(96/100) & Loss 242.89053344726562\n",
            "Epochs(97/100) & Loss 240.45285034179688\n",
            "Epochs(98/100) & Loss 238.0447998046875\n",
            "Epochs(99/100) & Loss 235.6659393310547\n",
            "Epochs(100/100) & Loss 233.3160858154297\n",
            "Epochs(101/100) & Loss 230.99472045898438\n",
            "Epochs(102/100) & Loss 228.70166015625\n",
            "Epochs(103/100) & Loss 226.4361572265625\n",
            "Epochs(104/100) & Loss 224.1983642578125\n",
            "Epochs(105/100) & Loss 221.9875946044922\n",
            "Epochs(106/100) & Loss 219.80368041992188\n",
            "Epochs(107/100) & Loss 217.64620971679688\n",
            "Epochs(108/100) & Loss 215.514892578125\n",
            "Epochs(109/100) & Loss 213.40927124023438\n",
            "Epochs(110/100) & Loss 211.32925415039062\n",
            "Epochs(111/100) & Loss 209.27426147460938\n",
            "Epochs(112/100) & Loss 207.244140625\n",
            "Epochs(113/100) & Loss 205.2386474609375\n",
            "Epochs(114/100) & Loss 203.25735473632812\n",
            "Epochs(115/100) & Loss 201.2998504638672\n",
            "Epochs(116/100) & Loss 199.36605834960938\n",
            "Epochs(117/100) & Loss 197.45555114746094\n",
            "Epochs(118/100) & Loss 195.568115234375\n",
            "Epochs(119/100) & Loss 193.70339965820312\n",
            "Epochs(120/100) & Loss 191.86111450195312\n",
            "Epochs(121/100) & Loss 190.04100036621094\n",
            "Epochs(122/100) & Loss 188.24281311035156\n",
            "Epochs(123/100) & Loss 186.46620178222656\n",
            "Epochs(124/100) & Loss 184.71096801757812\n",
            "Epochs(125/100) & Loss 182.976806640625\n",
            "Epochs(126/100) & Loss 181.26344299316406\n",
            "Epochs(127/100) & Loss 179.5706787109375\n",
            "Epochs(128/100) & Loss 177.8981475830078\n",
            "Epochs(129/100) & Loss 176.2457733154297\n",
            "Epochs(130/100) & Loss 174.61306762695312\n",
            "Epochs(131/100) & Loss 172.99989318847656\n",
            "Epochs(132/100) & Loss 171.4061279296875\n",
            "Epochs(133/100) & Loss 169.83135986328125\n",
            "Epochs(134/100) & Loss 168.27545166015625\n",
            "Epochs(135/100) & Loss 166.7381134033203\n",
            "Epochs(136/100) & Loss 165.21910095214844\n",
            "Epochs(137/100) & Loss 163.7182159423828\n",
            "Epochs(138/100) & Loss 162.23526000976562\n",
            "Epochs(139/100) & Loss 160.7699432373047\n",
            "Epochs(140/100) & Loss 159.3220672607422\n",
            "Epochs(141/100) & Loss 157.8914337158203\n",
            "Epochs(142/100) & Loss 156.47784423828125\n",
            "Epochs(143/100) & Loss 155.08108520507812\n",
            "Epochs(144/100) & Loss 153.70077514648438\n",
            "Epochs(145/100) & Loss 152.33694458007812\n",
            "Epochs(146/100) & Loss 150.98928833007812\n",
            "Epochs(147/100) & Loss 149.65753173828125\n",
            "Epochs(148/100) & Loss 148.34173583984375\n",
            "Epochs(149/100) & Loss 147.0414276123047\n",
            "Epochs(150/100) & Loss 145.75645446777344\n",
            "Epochs(151/100) & Loss 144.48672485351562\n",
            "Epochs(152/100) & Loss 143.23191833496094\n",
            "Epochs(153/100) & Loss 141.99200439453125\n",
            "Epochs(154/100) & Loss 140.76673889160156\n",
            "Epochs(155/100) & Loss 139.55587768554688\n",
            "Epochs(156/100) & Loss 138.35931396484375\n",
            "Epochs(157/100) & Loss 137.1768341064453\n",
            "Epochs(158/100) & Loss 136.00827026367188\n",
            "Epochs(159/100) & Loss 134.85342407226562\n",
            "Epochs(160/100) & Loss 133.71212768554688\n",
            "Epochs(161/100) & Loss 132.5842742919922\n",
            "Epochs(162/100) & Loss 131.4696044921875\n",
            "Epochs(163/100) & Loss 130.36807250976562\n",
            "Epochs(164/100) & Loss 129.27938842773438\n",
            "Epochs(165/100) & Loss 128.20346069335938\n",
            "Epochs(166/100) & Loss 127.14013671875\n",
            "Epochs(167/100) & Loss 126.08918762207031\n",
            "Epochs(168/100) & Loss 125.05049896240234\n",
            "Epochs(169/100) & Loss 124.0240249633789\n",
            "Epochs(170/100) & Loss 123.00941467285156\n",
            "Epochs(171/100) & Loss 122.0067138671875\n",
            "Epochs(172/100) & Loss 121.01558685302734\n",
            "Epochs(173/100) & Loss 120.03602600097656\n",
            "Epochs(174/100) & Loss 119.0678482055664\n",
            "Epochs(175/100) & Loss 118.1109390258789\n",
            "Epochs(176/100) & Loss 117.16505432128906\n",
            "Epochs(177/100) & Loss 116.2302474975586\n",
            "Epochs(178/100) & Loss 115.30616760253906\n",
            "Epochs(179/100) & Loss 114.39276123046875\n",
            "Epochs(180/100) & Loss 113.48992919921875\n",
            "Epochs(181/100) & Loss 112.5975570678711\n",
            "Epochs(182/100) & Loss 111.71546936035156\n",
            "Epochs(183/100) & Loss 110.8435287475586\n",
            "Epochs(184/100) & Loss 109.98164367675781\n",
            "Epochs(185/100) & Loss 109.129638671875\n",
            "Epochs(186/100) & Loss 108.2874984741211\n",
            "Epochs(187/100) & Loss 107.4549789428711\n",
            "Epochs(188/100) & Loss 106.6319808959961\n",
            "Epochs(189/100) & Loss 105.8184814453125\n",
            "Epochs(190/100) & Loss 105.01422119140625\n",
            "Epochs(191/100) & Loss 104.21923828125\n",
            "Epochs(192/100) & Loss 103.433349609375\n",
            "Epochs(193/100) & Loss 102.65629577636719\n",
            "Epochs(194/100) & Loss 101.88822174072266\n",
            "Epochs(195/100) & Loss 101.1288833618164\n",
            "Epochs(196/100) & Loss 100.3781509399414\n",
            "Epochs(197/100) & Loss 99.6359634399414\n",
            "Epochs(198/100) & Loss 98.90215301513672\n",
            "Epochs(199/100) & Loss 98.1767349243164\n",
            "Epochs(200/100) & Loss 97.4594955444336\n",
            "Epochs(201/100) & Loss 96.75040435791016\n",
            "Epochs(202/100) & Loss 96.04927825927734\n",
            "Epochs(203/100) & Loss 95.35614013671875\n",
            "Epochs(204/100) & Loss 94.67073822021484\n",
            "Epochs(205/100) & Loss 93.99310302734375\n",
            "Epochs(206/100) & Loss 93.32311248779297\n",
            "Epochs(207/100) & Loss 92.66059875488281\n",
            "Epochs(208/100) & Loss 92.00556945800781\n",
            "Epochs(209/100) & Loss 91.35786437988281\n",
            "Epochs(210/100) & Loss 90.71739196777344\n",
            "Epochs(211/100) & Loss 90.08412170410156\n",
            "Epochs(212/100) & Loss 89.45796966552734\n",
            "Epochs(213/100) & Loss 88.83876037597656\n",
            "Epochs(214/100) & Loss 88.22644805908203\n",
            "Epochs(215/100) & Loss 87.62092590332031\n",
            "Epochs(216/100) & Loss 87.0221939086914\n",
            "Epochs(217/100) & Loss 86.43002319335938\n",
            "Epochs(218/100) & Loss 85.8445053100586\n",
            "Epochs(219/100) & Loss 85.26541900634766\n",
            "Epochs(220/100) & Loss 84.6927490234375\n",
            "Epochs(221/100) & Loss 84.12638854980469\n",
            "Epochs(222/100) & Loss 83.56632232666016\n",
            "Epochs(223/100) & Loss 83.01241302490234\n",
            "Epochs(224/100) & Loss 82.46456146240234\n",
            "Epochs(225/100) & Loss 81.92279052734375\n",
            "Epochs(226/100) & Loss 81.38694763183594\n",
            "Epochs(227/100) & Loss 80.85691833496094\n",
            "Epochs(228/100) & Loss 80.332763671875\n",
            "Epochs(229/100) & Loss 79.81421661376953\n",
            "Epochs(230/100) & Loss 79.30142974853516\n",
            "Epochs(231/100) & Loss 78.79418182373047\n",
            "Epochs(232/100) & Loss 78.29251098632812\n",
            "Epochs(233/100) & Loss 77.7962646484375\n",
            "Epochs(234/100) & Loss 77.30534362792969\n",
            "Epochs(235/100) & Loss 76.81980895996094\n",
            "Epochs(236/100) & Loss 76.3395004272461\n",
            "Epochs(237/100) & Loss 75.86438751220703\n",
            "Epochs(238/100) & Loss 75.39432525634766\n",
            "Epochs(239/100) & Loss 74.92938995361328\n",
            "Epochs(240/100) & Loss 74.46951293945312\n",
            "Epochs(241/100) & Loss 74.01448059082031\n",
            "Epochs(242/100) & Loss 73.56433868408203\n",
            "Epochs(243/100) & Loss 73.1190414428711\n",
            "Epochs(244/100) & Loss 72.67842864990234\n",
            "Epochs(245/100) & Loss 72.24256896972656\n",
            "Epochs(246/100) & Loss 71.81138610839844\n",
            "Epochs(247/100) & Loss 71.38467407226562\n",
            "Epochs(248/100) & Loss 70.96260070800781\n",
            "Epochs(249/100) & Loss 70.54493713378906\n",
            "Epochs(250/100) & Loss 70.13168334960938\n",
            "Epochs(251/100) & Loss 69.72283935546875\n",
            "Epochs(252/100) & Loss 69.3182601928711\n",
            "Epochs(253/100) & Loss 68.91797637939453\n",
            "Epochs(254/100) & Loss 68.52185821533203\n",
            "Epochs(255/100) & Loss 68.1298599243164\n",
            "Epochs(256/100) & Loss 67.74205017089844\n",
            "Epochs(257/100) & Loss 67.3582534790039\n",
            "Epochs(258/100) & Loss 66.97840881347656\n",
            "Epochs(259/100) & Loss 66.60257720947266\n",
            "Epochs(260/100) & Loss 66.23063659667969\n",
            "Epochs(261/100) & Loss 65.86251068115234\n",
            "Epochs(262/100) & Loss 65.49828338623047\n",
            "Epochs(263/100) & Loss 65.13774108886719\n",
            "Epochs(264/100) & Loss 64.78093719482422\n",
            "Epochs(265/100) & Loss 64.42781066894531\n",
            "Epochs(266/100) & Loss 64.07827758789062\n",
            "Epochs(267/100) & Loss 63.732398986816406\n",
            "Epochs(268/100) & Loss 63.3900032043457\n",
            "Epochs(269/100) & Loss 63.051109313964844\n",
            "Epochs(270/100) & Loss 62.7156867980957\n",
            "Epochs(271/100) & Loss 62.3836555480957\n",
            "Epochs(272/100) & Loss 62.05500030517578\n",
            "Epochs(273/100) & Loss 61.729652404785156\n",
            "Epochs(274/100) & Loss 61.40766143798828\n",
            "Epochs(275/100) & Loss 61.08879852294922\n",
            "Epochs(276/100) & Loss 60.77324295043945\n",
            "Epochs(277/100) & Loss 60.46088409423828\n",
            "Epochs(278/100) & Loss 60.15155792236328\n",
            "Epochs(279/100) & Loss 59.84537887573242\n",
            "Epochs(280/100) & Loss 59.5422248840332\n",
            "Epochs(281/100) & Loss 59.24208450317383\n",
            "Epochs(282/100) & Loss 58.94492721557617\n",
            "Epochs(283/100) & Loss 58.650787353515625\n",
            "Epochs(284/100) & Loss 58.3594970703125\n",
            "Epochs(285/100) & Loss 58.071128845214844\n",
            "Epochs(286/100) & Loss 57.785560607910156\n",
            "Epochs(287/100) & Loss 57.50278854370117\n",
            "Epochs(288/100) & Loss 57.2227783203125\n",
            "Epochs(289/100) & Loss 56.94550323486328\n",
            "Epochs(290/100) & Loss 56.67095184326172\n",
            "Epochs(291/100) & Loss 56.39905548095703\n",
            "Epochs(292/100) & Loss 56.12983322143555\n",
            "Epochs(293/100) & Loss 55.86322784423828\n",
            "Epochs(294/100) & Loss 55.59914016723633\n",
            "Epochs(295/100) & Loss 55.33763885498047\n",
            "Epochs(296/100) & Loss 55.078643798828125\n",
            "Epochs(297/100) & Loss 54.8221321105957\n",
            "Epochs(298/100) & Loss 54.568077087402344\n",
            "Epochs(299/100) & Loss 54.31647491455078\n",
            "Epochs(300/100) & Loss 54.0672607421875\n",
            "Epochs(301/100) & Loss 53.820350646972656\n",
            "Epochs(302/100) & Loss 53.5758056640625\n",
            "Epochs(303/100) & Loss 53.33363723754883\n",
            "Epochs(304/100) & Loss 53.093650817871094\n",
            "Epochs(305/100) & Loss 52.855987548828125\n",
            "Epochs(306/100) & Loss 52.620574951171875\n",
            "Epochs(307/100) & Loss 52.38727569580078\n",
            "Epochs(308/100) & Loss 52.15623092651367\n",
            "Epochs(309/100) & Loss 51.927284240722656\n",
            "Epochs(310/100) & Loss 51.70048904418945\n",
            "Epochs(311/100) & Loss 51.47576904296875\n",
            "Epochs(312/100) & Loss 51.253135681152344\n",
            "Epochs(313/100) & Loss 51.0326042175293\n",
            "Epochs(314/100) & Loss 50.81403350830078\n",
            "Epochs(315/100) & Loss 50.597469329833984\n",
            "Epochs(316/100) & Loss 50.382904052734375\n",
            "Epochs(317/100) & Loss 50.17027282714844\n",
            "Epochs(318/100) & Loss 49.95957565307617\n",
            "Epochs(319/100) & Loss 49.75075149536133\n",
            "Epochs(320/100) & Loss 49.54385757446289\n",
            "Epochs(321/100) & Loss 49.33882141113281\n",
            "Epochs(322/100) & Loss 49.135597229003906\n",
            "Epochs(323/100) & Loss 48.9341926574707\n",
            "Epochs(324/100) & Loss 48.73461151123047\n",
            "Epochs(325/100) & Loss 48.53681182861328\n",
            "Epochs(326/100) & Loss 48.34074020385742\n",
            "Epochs(327/100) & Loss 48.14641571044922\n",
            "Epochs(328/100) & Loss 47.953792572021484\n",
            "Epochs(329/100) & Loss 47.76283645629883\n",
            "Epochs(330/100) & Loss 47.573570251464844\n",
            "Epochs(331/100) & Loss 47.38599395751953\n",
            "Epochs(332/100) & Loss 47.19998550415039\n",
            "Epochs(333/100) & Loss 47.01561737060547\n",
            "Epochs(334/100) & Loss 46.832862854003906\n",
            "Epochs(335/100) & Loss 46.65167236328125\n",
            "Epochs(336/100) & Loss 46.472007751464844\n",
            "Epochs(337/100) & Loss 46.29396057128906\n",
            "Epochs(338/100) & Loss 46.11741638183594\n",
            "Epochs(339/100) & Loss 45.94230651855469\n",
            "Epochs(340/100) & Loss 45.76875686645508\n",
            "Epochs(341/100) & Loss 45.596622467041016\n",
            "Epochs(342/100) & Loss 45.42596435546875\n",
            "Epochs(343/100) & Loss 45.256710052490234\n",
            "Epochs(344/100) & Loss 45.08887481689453\n",
            "Epochs(345/100) & Loss 44.922462463378906\n",
            "Epochs(346/100) & Loss 44.7574577331543\n",
            "Epochs(347/100) & Loss 44.593780517578125\n",
            "Epochs(348/100) & Loss 44.43147277832031\n",
            "Epochs(349/100) & Loss 44.27048110961914\n",
            "Epochs(350/100) & Loss 44.11084747314453\n",
            "Epochs(351/100) & Loss 43.952491760253906\n",
            "Epochs(352/100) & Loss 43.795433044433594\n",
            "Epochs(353/100) & Loss 43.63969802856445\n",
            "Epochs(354/100) & Loss 43.48517990112305\n",
            "Epochs(355/100) & Loss 43.33186721801758\n",
            "Epochs(356/100) & Loss 43.179813385009766\n",
            "Epochs(357/100) & Loss 43.02897262573242\n",
            "Epochs(358/100) & Loss 42.879371643066406\n",
            "Epochs(359/100) & Loss 42.730926513671875\n",
            "Epochs(360/100) & Loss 42.583702087402344\n",
            "Epochs(361/100) & Loss 42.43757247924805\n",
            "Epochs(362/100) & Loss 42.29264450073242\n",
            "Epochs(363/100) & Loss 42.14876174926758\n",
            "Epochs(364/100) & Loss 42.00614547729492\n",
            "Epochs(365/100) & Loss 41.86454391479492\n",
            "Epochs(366/100) & Loss 41.72407913208008\n",
            "Epochs(367/100) & Loss 41.58464050292969\n",
            "Epochs(368/100) & Loss 41.44635772705078\n",
            "Epochs(369/100) & Loss 41.30908966064453\n",
            "Epochs(370/100) & Loss 41.17282485961914\n",
            "Epochs(371/100) & Loss 41.03766632080078\n",
            "Epochs(372/100) & Loss 40.90349578857422\n",
            "Epochs(373/100) & Loss 40.770362854003906\n",
            "Epochs(374/100) & Loss 40.63824462890625\n",
            "Epochs(375/100) & Loss 40.50700759887695\n",
            "Epochs(376/100) & Loss 40.376853942871094\n",
            "Epochs(377/100) & Loss 40.24763870239258\n",
            "Epochs(378/100) & Loss 40.119407653808594\n",
            "Epochs(379/100) & Loss 39.99208068847656\n",
            "Epochs(380/100) & Loss 39.86571502685547\n",
            "Epochs(381/100) & Loss 39.74024200439453\n",
            "Epochs(382/100) & Loss 39.61573028564453\n",
            "Epochs(383/100) & Loss 39.492088317871094\n",
            "Epochs(384/100) & Loss 39.36933517456055\n",
            "Epochs(385/100) & Loss 39.24744415283203\n",
            "Epochs(386/100) & Loss 39.12648391723633\n",
            "Epochs(387/100) & Loss 39.00637435913086\n",
            "Epochs(388/100) & Loss 38.88710021972656\n",
            "Epochs(389/100) & Loss 38.76865768432617\n",
            "Epochs(390/100) & Loss 38.65106964111328\n",
            "Epochs(391/100) & Loss 38.53428649902344\n",
            "Epochs(392/100) & Loss 38.41838073730469\n",
            "Epochs(393/100) & Loss 38.30323028564453\n",
            "Epochs(394/100) & Loss 38.18885803222656\n",
            "Epochs(395/100) & Loss 38.07532501220703\n",
            "Epochs(396/100) & Loss 37.962562561035156\n",
            "Epochs(397/100) & Loss 37.85054016113281\n",
            "Epochs(398/100) & Loss 37.73931121826172\n",
            "Epochs(399/100) & Loss 37.62879943847656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = model(inputs)\n",
        "loss = MSE(target, preds)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTJjZlNQ-YGS",
        "outputId": "a10a1f1e-208d-4e43-b9bc-9f81037fcbd1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(37.5191, grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import sqrt\n",
        "sqrt(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOQeSZeM-a-L",
        "outputId": "929e0d24-db48-4c17-f960-4bec175e85cf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.125283059566672"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "317vvItO-dMF",
        "outputId": "aa8021a3-455e-4ad5-98e5-c52fbd63fc06"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 58.2879,  67.9695],\n",
              "        [ 77.4008,  99.7453],\n",
              "        [127.8128, 114.0794],\n",
              "        [ 28.8271,  41.2108],\n",
              "        [ 88.6782, 117.2619]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3TbANjcw-e9f",
        "outputId": "e549fcc2-b1ce-4c3c-a5af-dd26dadc0784"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 56.,  70.],\n",
              "        [ 81., 101.],\n",
              "        [119., 113.],\n",
              "        [ 22.,  37.],\n",
              "        [103., 119.]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gATg3a3r-h4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose"
      ],
      "metadata": {
        "id": "gfZ6FQV6BTuu"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download training data from open datasets\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root = 'data',\n",
        "    train= True,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download = True,\n",
        "    transform = ToTensor(),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4S_FNuD0BUY4",
        "outputId": "fbfda4ba-fd01-486f-a5e6-72ee7937559a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26.4M/26.4M [00:01<00:00, 17.2MB/s]\n",
            "100%|██████████| 29.5k/29.5k [00:00<00:00, 276kB/s]\n",
            "100%|██████████| 4.42M/4.42M [00:00<00:00, 5.08MB/s]\n",
            "100%|██████████| 5.15k/5.15k [00:00<00:00, 12.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "pJmEfHbQBdQo",
        "outputId": "5f83a7aa-880e-492a-8571-570971d80474"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchvision.datasets.mnist.FashionMNIST"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>torchvision.datasets.mnist.FashionMNIST</b><br/>def __init__(root: Union[str, Path], train: bool=True, transform: Optional[Callable]=None, target_transform: Optional[Callable]=None, download: bool=False) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.12/dist-packages/torchvision/datasets/mnist.py</a>`Fashion-MNIST &lt;https://github.com/zalandoresearch/fashion-mnist&gt;`_ Dataset.\n",
              "\n",
              "Args:\n",
              "    root (str or ``pathlib.Path``): Root directory of dataset where ``FashionMNIST/raw/train-images-idx3-ubyte``\n",
              "        and  ``FashionMNIST/raw/t10k-images-idx3-ubyte`` exist.\n",
              "    train (bool, optional): If True, creates dataset from ``train-images-idx3-ubyte``,\n",
              "        otherwise from ``t10k-images-idx3-ubyte``.\n",
              "    transform (callable, optional): A function/transform that  takes in a PIL image\n",
              "        and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
              "    target_transform (callable, optional): A function/transform that takes in the\n",
              "        target and transforms it.\n",
              "    download (bool, optional): If True, downloads the dataset from the internet and\n",
              "        puts it in root directory. If dataset is already downloaded, it is not\n",
              "        downloaded again.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 204);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size = batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size = batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "  print(\"Shape of X [N, C, H, W] \", X.shape)\n",
        "  print(\"Shape of y: \", y.shape, y.dtype)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8SmVRMvBiQu",
        "outputId": "813c8c48-9df2-4ddf-f797-6427a4855c05"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]  torch.Size([64, 1, 28, 28])\n",
            "Shape of y:  torch.Size([64]) torch.int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get cpu or gpu device for training\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1psnbuUdBk4o",
        "outputId": "5f73d77a-c53c-4ad3-f14d-996ae48aa51b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the NN Model\n",
        "class NeuralNetwork(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    # Hidden Layers with ReLU activation function\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 10) # Output layer\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits\n",
        "\n",
        "\n",
        "model= NeuralNetwork().to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icK6MEDvBnJ8",
        "outputId": "a6a3016a-959d-42a2-b3ea-d6f68fc56b3f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy Loss ----> Because it is a multiclass classification problem\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer ---> SGD ---> Stochastic Gradient Descent\n",
        "# lr = Learning Rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr = 1e-3)"
      ],
      "metadata": {
        "id": "uVoYin2RBsus"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  model.train()\n",
        "\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    X, y= X.to(device), y.to(device) # related to gpu computation\n",
        "\n",
        "    # Compute prediction error\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # BackPropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch %100 ==0 :\n",
        "      loss, current = loss.item(), batch * len(X)\n",
        "      print(f\"Loss: {loss} [{current}/{size}]\")"
      ],
      "metadata": {
        "id": "IoUq19g9Bv3g"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "\n",
        "  num_batches = len(dataloader)\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "      pred = model(X)\n",
        "\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches # average loss per batch\n",
        "  correct /= size # %age of correct predictions or accuracy\n",
        "\n",
        "  print(f\"Test Error: \\n Accuracy: {100*correct} %, Avg loss {test_loss}\\n\")"
      ],
      "metadata": {
        "id": "SRJ45OIYByEJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 1\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1} \\n --------------------------\")\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "\n",
        "print(\"Done\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stuB5xeQB0KQ",
        "outputId": "833b0d64-8cb3-4262-81d4-e4923251e687"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \n",
            " --------------------------\n",
            "Loss: 2.289262533187866 [0/60000]\n",
            "Loss: 2.2879104614257812 [6400/60000]\n",
            "Loss: 2.266846179962158 [12800/60000]\n",
            "Loss: 2.262615919113159 [19200/60000]\n",
            "Loss: 2.241848945617676 [25600/60000]\n",
            "Loss: 2.2138566970825195 [32000/60000]\n",
            "Loss: 2.224708080291748 [38400/60000]\n",
            "Loss: 2.190427780151367 [44800/60000]\n",
            "Loss: 2.1818764209747314 [51200/60000]\n",
            "Loss: 2.1458475589752197 [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 39.11 %, Avg loss 2.1473802305330896\n",
            "\n",
            "Done\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#save model\n",
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved model state to model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gggqFwriB5nk",
        "outputId": "3d13906e-d28a-4d24-85d7-2ea5f25247dd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model state to model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Prediction\n",
        "\n",
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "\n",
        "\"Trouser\",\n",
        "\n",
        "\"Pullover\",\n",
        "\n",
        "\"Dress\",\n",
        "\n",
        "\"Coat\",\n",
        "\n",
        "\"Sandal\",\n",
        "\n",
        "\"Shirt\",\n",
        "\n",
        "\"Sneaker\",\n",
        "\n",
        "\"Bag\",\n",
        "\n",
        "\"Ankle boot\"\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "x, y = test_data[10][0], test_data[10][1]\n",
        "x = x.to(device)\n",
        "# y = y.to(device)\n",
        "with torch.no_grad():\n",
        "  pred = model(x)\n",
        "  predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "\n",
        "  print(f\"Predicted: {predicted} Actual: {actual}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL6RR74XB_fF",
        "outputId": "a112a4e0-c440-45b6-a021-935a280f2fc5"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: Coat Actual: Coat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'x' contains the image data\n",
        "plt.imshow(x.cpu().squeeze())  # Move tensor to CPU if using GPU\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "c2mG4tn9CD7f",
        "outputId": "50fee1ae-7a39-4541-866e-b8091d73bb0f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VDatbpE0CH1M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}